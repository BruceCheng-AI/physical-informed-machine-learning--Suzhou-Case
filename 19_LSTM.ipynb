{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM初始化"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Normalize your data\n",
    "# This step is optional and depends on the nature of your dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_target = StandardScaler()\n",
    "scaler_history = StandardScaler()  # Define scaler_history object\n",
    "\n",
    "# load data\n",
    "history = pd.read_csv('../data/results/history.csv', index_col='time',parse_dates=True)\n",
    "ssp245 = pd.read_csv('../data/results/ssp245.csv', index_col=0)\n",
    "ssp585 = pd.read_csv('../data/results/ssp585.csv', index_col=0)\n",
    "history_wl = pd.read_csv('../data/results/history_wl.csv', index_col='time',parse_dates=True)\n",
    "\n",
    "# Convert history data to scaled values\n",
    "history_scaled = scaler_history.fit_transform(history)\n",
    "\n",
    "target_scaled = scaler_target.fit_transform(history_wl)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "history_tensor = torch.Tensor(history_scaled)\n",
    "target_tensor = torch.Tensor(target_scaled)\n",
    "\n",
    "# Split the dataset into training and testing datasets\n",
    "dataset = TensorDataset(history_tensor, target_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Define a DataLoader for training and testing datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # predicting 1 value\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = history_tensor.shape[1]\n",
    "hidden_size = 50\n",
    "num_layers = 2\n",
    "\n",
    "model = LSTM(input_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "\n",
    "# Define Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Reshape input tensor to [batch_size, sequence_length, n_features]\n",
    "        inputs = inputs.reshape(-1, 1, input_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 2 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_history_tensors = []\n",
    "test_target_tensors = []\n",
    "for inputs, targets in test_loader:\n",
    "    test_history_tensors.append(inputs)\n",
    "    test_target_tensors.append(targets)\n",
    "\n",
    "test_history_tensor = torch.cat(test_history_tensors, dim=0)\n",
    "test_target_tensor = torch.cat(test_target_tensors, dim=0)\n",
    "\n",
    "# Make predictions on the test set\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Do not compute gradient to save memory\n",
    "    test_predictions = model(test_history_tensor.reshape(-1, 1, input_size))\n",
    "\n",
    "# Compute evaluation metrics\n",
    "test_rmse = np.sqrt(mean_squared_error(test_target_tensor.numpy(), test_predictions.numpy()))\n",
    "test_r2 = r2_score(test_target_tensor.numpy(), test_predictions.numpy())\n",
    "\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "print(f\"Test R2 score: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now suppose you have two future datasets SSP245 and SSP585\n",
    "# They should be PyTorch tensors and have the same shape as the input\n",
    "SSP245_tensor = torch.Tensor(scaler_history.transform(ssp245))  # Replace this line with your code if necessary\n",
    "SSP585_tensor = torch.Tensor(scaler_history.transform(ssp585))  # Replace this line with your code if necessary\n",
    "\n",
    "# We can use the trained model to predict the future\n",
    "SSP245_predictions = model(SSP245_tensor.to(device).reshape(-1, 1, input_size))\n",
    "SSP585_predictions = model(SSP585_tensor.to(device).reshape(-1, 1, input_size))\n",
    "\n",
    "# If your data was scaled, you might want to rescale the predictions back to the original scale\n",
    "SSP245_predictions_rescaled = scaler_target.inverse_transform(SSP245_predictions.detach().numpy())\n",
    "SSP585_predictions_rescaled = scaler_target.inverse_transform(SSP585_predictions.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with hidden_size=50, num_layers=1, learning_rate=0.1\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.3021\n",
      "Epoch [4/10], Loss: 0.4437\n",
      "Epoch [6/10], Loss: 0.4912\n",
      "Epoch [8/10], Loss: 0.4524\n",
      "Epoch [10/10], Loss: 0.4505\n",
      "Test RMSE: 0.8381, Test R2 score: 0.3031\n",
      "Training model with hidden_size=50, num_layers=1, learning_rate=0.01\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.2752\n",
      "Epoch [4/10], Loss: 0.2998\n",
      "Epoch [6/10], Loss: 0.3211\n",
      "Epoch [8/10], Loss: 0.3352\n",
      "Epoch [10/10], Loss: 0.3393\n",
      "Test RMSE: 0.7732, Test R2 score: 0.4070\n",
      "Training model with hidden_size=50, num_layers=1, learning_rate=0.001\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.2836\n",
      "Epoch [4/10], Loss: 0.2556\n",
      "Epoch [6/10], Loss: 0.2567\n",
      "Epoch [8/10], Loss: 0.2605\n",
      "Epoch [10/10], Loss: 0.2631\n",
      "Test RMSE: 0.7959, Test R2 score: 0.3716\n",
      "Training model with hidden_size=50, num_layers=2, learning_rate=0.1\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.4066\n",
      "Epoch [4/10], Loss: 0.3115\n",
      "Epoch [6/10], Loss: 0.4881\n",
      "Epoch [8/10], Loss: 0.2967\n",
      "Epoch [10/10], Loss: 0.5018\n",
      "Test RMSE: 0.8782, Test R2 score: 0.2349\n",
      "Training model with hidden_size=50, num_layers=2, learning_rate=0.01\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.2574\n",
      "Epoch [4/10], Loss: 0.2989\n",
      "Epoch [6/10], Loss: 0.2963\n",
      "Epoch [8/10], Loss: 0.3091\n",
      "Epoch [10/10], Loss: 0.2867\n",
      "Test RMSE: 0.7656, Test R2 score: 0.4186\n",
      "Training model with hidden_size=50, num_layers=2, learning_rate=0.001\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.2693\n",
      "Epoch [4/10], Loss: 0.2457\n",
      "Epoch [6/10], Loss: 0.2359\n",
      "Epoch [8/10], Loss: 0.2335\n",
      "Epoch [10/10], Loss: 0.2373\n",
      "Test RMSE: 0.7858, Test R2 score: 0.3874\n",
      "Training model with hidden_size=50, num_layers=3, learning_rate=0.1\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.5861\n",
      "Epoch [4/10], Loss: 0.5617\n",
      "Epoch [6/10], Loss: 0.7363\n",
      "Epoch [8/10], Loss: 0.4981\n",
      "Epoch [10/10], Loss: 0.4484\n",
      "Test RMSE: 0.8636, Test R2 score: 0.2601\n",
      "Training model with hidden_size=50, num_layers=3, learning_rate=0.01\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.2704\n",
      "Epoch [4/10], Loss: 0.3254\n",
      "Epoch [6/10], Loss: 0.3045\n",
      "Epoch [8/10], Loss: 0.3028\n",
      "Epoch [10/10], Loss: 0.3382\n",
      "Test RMSE: 0.7028, Test R2 score: 0.5100\n",
      "Training model with hidden_size=50, num_layers=3, learning_rate=0.001\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.2449\n",
      "Epoch [4/10], Loss: 0.2249\n",
      "Epoch [6/10], Loss: 0.2164\n",
      "Epoch [8/10], Loss: 0.2125\n",
      "Epoch [10/10], Loss: 0.2159\n",
      "Test RMSE: 0.7872, Test R2 score: 0.3852\n",
      "Training model with hidden_size=50, num_layers=4, learning_rate=0.1\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.5010\n",
      "Epoch [4/10], Loss: 0.2774\n",
      "Epoch [6/10], Loss: 0.4220\n",
      "Epoch [8/10], Loss: 0.3820\n",
      "Epoch [10/10], Loss: 0.5789\n",
      "Test RMSE: 0.9233, Test R2 score: 0.1543\n",
      "Training model with hidden_size=50, num_layers=4, learning_rate=0.01\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.2780\n",
      "Epoch [4/10], Loss: 0.3035\n",
      "Epoch [6/10], Loss: 0.3531\n",
      "Epoch [8/10], Loss: 0.4116\n",
      "Epoch [10/10], Loss: 0.4253\n",
      "Test RMSE: 0.7839, Test R2 score: 0.3904\n",
      "Training model with hidden_size=50, num_layers=4, learning_rate=0.001\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.2506\n",
      "Epoch [4/10], Loss: 0.2262\n",
      "Epoch [6/10], Loss: 0.2191\n",
      "Epoch [8/10], Loss: 0.2200\n",
      "Epoch [10/10], Loss: 0.2274\n",
      "Test RMSE: 0.7908, Test R2 score: 0.3796\n",
      "Training model with hidden_size=100, num_layers=1, learning_rate=0.1\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.3267\n",
      "Epoch [4/10], Loss: 0.5324\n",
      "Epoch [6/10], Loss: 0.3380\n",
      "Epoch [8/10], Loss: 0.3572\n",
      "Epoch [10/10], Loss: 0.3047\n",
      "Test RMSE: 0.8063, Test R2 score: 0.3551\n",
      "Training model with hidden_size=100, num_layers=1, learning_rate=0.01\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.2805\n",
      "Epoch [4/10], Loss: 0.3265\n",
      "Epoch [6/10], Loss: 0.3128\n",
      "Epoch [8/10], Loss: 0.3260\n",
      "Epoch [10/10], Loss: 0.3311\n",
      "Test RMSE: 0.7730, Test R2 score: 0.4072\n",
      "Training model with hidden_size=100, num_layers=1, learning_rate=0.001\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.2839\n",
      "Epoch [4/10], Loss: 0.2589\n",
      "Epoch [6/10], Loss: 0.2584\n",
      "Epoch [8/10], Loss: 0.2599\n",
      "Epoch [10/10], Loss: 0.2614\n",
      "Test RMSE: 0.7959, Test R2 score: 0.3716\n",
      "Training model with hidden_size=100, num_layers=2, learning_rate=0.1\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.4999\n",
      "Epoch [4/10], Loss: 0.4263\n",
      "Epoch [6/10], Loss: 0.4917\n",
      "Epoch [8/10], Loss: 0.6630\n",
      "Epoch [10/10], Loss: 0.4105\n",
      "Test RMSE: 0.8922, Test R2 score: 0.2103\n",
      "Training model with hidden_size=100, num_layers=2, learning_rate=0.01\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.2656\n",
      "Epoch [4/10], Loss: 0.3395\n",
      "Epoch [6/10], Loss: 0.3656\n",
      "Epoch [8/10], Loss: 0.3431\n",
      "Epoch [10/10], Loss: 0.3626\n",
      "Test RMSE: 0.7467, Test R2 score: 0.4469\n",
      "Training model with hidden_size=100, num_layers=2, learning_rate=0.001\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.2671\n",
      "Epoch [4/10], Loss: 0.2505\n",
      "Epoch [6/10], Loss: 0.2426\n",
      "Epoch [8/10], Loss: 0.2377\n",
      "Epoch [10/10], Loss: 0.2385\n",
      "Test RMSE: 0.7899, Test R2 score: 0.3811\n",
      "Training model with hidden_size=100, num_layers=3, learning_rate=0.1\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.5960\n",
      "Epoch [4/10], Loss: 0.2928\n",
      "Epoch [6/10], Loss: 0.7728\n",
      "Epoch [8/10], Loss: 0.4865\n",
      "Epoch [10/10], Loss: 0.4959\n",
      "Test RMSE: 0.8747, Test R2 score: 0.2410\n",
      "Training model with hidden_size=100, num_layers=3, learning_rate=0.01\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.2339\n",
      "Epoch [4/10], Loss: 0.2827\n",
      "Epoch [6/10], Loss: 0.2867\n",
      "Epoch [8/10], Loss: 0.3248\n",
      "Epoch [10/10], Loss: 0.3568\n",
      "Test RMSE: 0.7381, Test R2 score: 0.4596\n",
      "Training model with hidden_size=100, num_layers=3, learning_rate=0.001\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.2566\n",
      "Epoch [4/10], Loss: 0.2459\n",
      "Epoch [6/10], Loss: 0.2449\n",
      "Epoch [8/10], Loss: 0.2439\n",
      "Epoch [10/10], Loss: 0.2453\n",
      "Test RMSE: 0.7867, Test R2 score: 0.3861\n",
      "Training model with hidden_size=100, num_layers=4, learning_rate=0.1\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.7151\n",
      "Epoch [4/10], Loss: 0.6424\n",
      "Epoch [6/10], Loss: 0.5275\n",
      "Epoch [8/10], Loss: 0.4532\n",
      "Epoch [10/10], Loss: 0.4193\n",
      "Test RMSE: 0.9683, Test R2 score: 0.0699\n",
      "Training model with hidden_size=100, num_layers=4, learning_rate=0.01\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.2402\n",
      "Epoch [4/10], Loss: 0.2700\n",
      "Epoch [6/10], Loss: 0.3215\n",
      "Epoch [8/10], Loss: 0.4172\n",
      "Epoch [10/10], Loss: 0.4253\n",
      "Test RMSE: 0.7691, Test R2 score: 0.4132\n",
      "Training model with hidden_size=100, num_layers=4, learning_rate=0.001\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.2552\n",
      "Epoch [4/10], Loss: 0.2422\n",
      "Epoch [6/10], Loss: 0.2386\n",
      "Epoch [8/10], Loss: 0.2399\n",
      "Epoch [10/10], Loss: 0.2429\n",
      "Test RMSE: 0.7874, Test R2 score: 0.3850\n",
      "Training model with hidden_size=150, num_layers=1, learning_rate=0.1\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.6052\n",
      "Epoch [4/10], Loss: 0.5974\n",
      "Epoch [6/10], Loss: 0.3911\n",
      "Epoch [8/10], Loss: 0.3306\n",
      "Epoch [10/10], Loss: 0.2857\n",
      "Test RMSE: 0.8410, Test R2 score: 0.2984\n",
      "Training model with hidden_size=150, num_layers=1, learning_rate=0.01\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.2699\n",
      "Epoch [4/10], Loss: 0.3070\n",
      "Epoch [6/10], Loss: 0.3284\n",
      "Epoch [8/10], Loss: 0.3382\n",
      "Epoch [10/10], Loss: 0.3326\n",
      "Test RMSE: 0.7720, Test R2 score: 0.4087\n",
      "Training model with hidden_size=150, num_layers=1, learning_rate=0.001\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.2857\n",
      "Epoch [4/10], Loss: 0.2638\n",
      "Epoch [6/10], Loss: 0.2609\n",
      "Epoch [8/10], Loss: 0.2594\n",
      "Epoch [10/10], Loss: 0.2598\n",
      "Test RMSE: 0.7944, Test R2 score: 0.3740\n",
      "Training model with hidden_size=150, num_layers=2, learning_rate=0.1\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.4312\n",
      "Epoch [4/10], Loss: 0.5867\n",
      "Epoch [6/10], Loss: 0.6834\n",
      "Epoch [8/10], Loss: 0.3431\n",
      "Epoch [10/10], Loss: 0.5901\n",
      "Test RMSE: 0.8730, Test R2 score: 0.2439\n",
      "Training model with hidden_size=150, num_layers=2, learning_rate=0.01\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.2918\n",
      "Epoch [4/10], Loss: 0.3197\n",
      "Epoch [6/10], Loss: 0.3123\n",
      "Epoch [8/10], Loss: 0.2480\n",
      "Epoch [10/10], Loss: 0.2545\n",
      "Test RMSE: 0.7585, Test R2 score: 0.4293\n",
      "Training model with hidden_size=150, num_layers=2, learning_rate=0.001\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.2632\n",
      "Epoch [4/10], Loss: 0.2532\n",
      "Epoch [6/10], Loss: 0.2482\n",
      "Epoch [8/10], Loss: 0.2458\n",
      "Epoch [10/10], Loss: 0.2486\n",
      "Test RMSE: 0.7906, Test R2 score: 0.3799\n",
      "Training model with hidden_size=150, num_layers=3, learning_rate=0.1\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.4029\n",
      "Epoch [4/10], Loss: 0.5902\n",
      "Epoch [6/10], Loss: 0.6963\n",
      "Epoch [8/10], Loss: 0.7237\n",
      "Epoch [10/10], Loss: 0.8273\n",
      "Test RMSE: 0.9544, Test R2 score: 0.0963\n",
      "Training model with hidden_size=150, num_layers=3, learning_rate=0.01\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.2826\n",
      "Epoch [4/10], Loss: 0.2926\n",
      "Epoch [6/10], Loss: 0.3393\n",
      "Epoch [8/10], Loss: 0.4046\n",
      "Epoch [10/10], Loss: 0.5596\n",
      "Test RMSE: 0.7767, Test R2 score: 0.4016\n",
      "Training model with hidden_size=150, num_layers=3, learning_rate=0.001\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.2596\n",
      "Epoch [4/10], Loss: 0.2493\n",
      "Epoch [6/10], Loss: 0.2453\n",
      "Epoch [8/10], Loss: 0.2455\n",
      "Epoch [10/10], Loss: 0.2490\n",
      "Test RMSE: 0.7854, Test R2 score: 0.3881\n",
      "Training model with hidden_size=150, num_layers=4, learning_rate=0.1\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.6963\n",
      "Epoch [4/10], Loss: 0.6695\n",
      "Epoch [6/10], Loss: 0.5849\n",
      "Epoch [8/10], Loss: 0.5036\n",
      "Epoch [10/10], Loss: 0.6405\n",
      "Test RMSE: 1.3693, Test R2 score: -0.8600\n",
      "Training model with hidden_size=150, num_layers=4, learning_rate=0.01\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.2476\n",
      "Epoch [4/10], Loss: 0.2702\n",
      "Epoch [6/10], Loss: 0.3696\n",
      "Epoch [8/10], Loss: 0.4172\n",
      "Epoch [10/10], Loss: 0.3623\n",
      "Test RMSE: 0.7571, Test R2 score: 0.4314\n",
      "Training model with hidden_size=150, num_layers=4, learning_rate=0.001\n",
      "Let's use 4 GPUs!\n",
      "Epoch [2/10], Loss: 0.2638\n",
      "Epoch [4/10], Loss: 0.2549\n",
      "Epoch [6/10], Loss: 0.2457\n",
      "Epoch [8/10], Loss: 0.2421\n",
      "Epoch [10/10], Loss: 0.2437\n",
      "Test RMSE: 0.7864, Test R2 score: 0.3864\n",
      "Best parameters are hidden_size=50, num_layers=3, learning_rate=0.01\n",
      "Best test RMSE: 0.7028, Best test R2 score: 0.5100\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Reshape input tensor to [batch_size, sequence_length, n_features]\n",
    "            inputs = inputs.reshape(-1, 1, input_size)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if (epoch+1) % 2 == 0:\n",
    "            print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "\n",
    "def tune_parameters(hidden_sizes, num_layers_list, learning_rates, num_epochs=10):\n",
    "    best_rmse = float('inf')\n",
    "    best_r2 = float('-inf')\n",
    "    best_params = None\n",
    "\n",
    "    for hidden_size in hidden_sizes:\n",
    "        for num_layers in num_layers_list:\n",
    "            for lr in learning_rates:\n",
    "                print(f\"Training model with hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={lr}\")\n",
    "                model = LSTM(input_size, hidden_size, num_layers).to(device)\n",
    "                if torch.cuda.device_count() > 1:\n",
    "                    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "                    model = nn.DataParallel(model)\n",
    "                criterion = nn.MSELoss()\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "                train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "                # Evaluate the model on the test set\n",
    "                model.eval()  # Set the model to evaluation mode\n",
    "                with torch.no_grad():  # Do not compute gradient to save memory\n",
    "                    test_predictions = model(test_history_tensor.reshape(-1, 1, input_size))\n",
    "\n",
    "                # Compute evaluation metrics\n",
    "                test_rmse = np.sqrt(mean_squared_error(test_target_tensor.cpu().numpy(), test_predictions.cpu().numpy()))\n",
    "                test_r2 = r2_score(test_target_tensor.cpu().numpy(), test_predictions.cpu().numpy())\n",
    "\n",
    "                print(f\"Test RMSE: {test_rmse:.4f}, Test R2 score: {test_r2:.4f}\")\n",
    "\n",
    "                # Update best params if current model is better\n",
    "                if test_rmse < best_rmse and test_r2 > best_r2:\n",
    "                    best_rmse = test_rmse\n",
    "                    best_r2 = test_r2\n",
    "                    best_params = (hidden_size, num_layers, lr)\n",
    "\n",
    "    print(f\"Best parameters are hidden_size={best_params[0]}, num_layers={best_params[1]}, learning_rate={best_params[2]}\")\n",
    "    print(f\"Best test RMSE: {best_rmse:.4f}, Best test R2 score: {best_r2:.4f}\")\n",
    "\n",
    "\n",
    "hidden_sizes = [50, 100, 150]\n",
    "num_layers_list = [1, 2, 3, 4]\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "tune_parameters(hidden_sizes, num_layers_list, learning_rates, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
